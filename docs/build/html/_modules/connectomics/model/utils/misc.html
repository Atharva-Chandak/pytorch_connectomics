



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>connectomics.model.utils.misc &mdash; connectomics latest documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.svg"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/css/pytc-theme.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css@1.0.0-alpha.28/dist/style.min.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">

    <a class="header-logo" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/index.html" aria-label="PyTorch"></a>

    <div class="header-container">

      <div class="main-menu">
        <ul>
          <li>
            <a href="../../../../index.html">Get Started</a>
          </li>
          <li>
            <a href="../../../../tutorials/snemi.html">Tutorials</a>
          </li>
          <li>
            <a href="../../../../index.html">Docs</a>
          </li>
          <li>
            <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master">Github</a>
          </li>
          <li>
            <a href="../../../../about.html">About Us</a>
          </li>

        </ul>
      </div>

      <!-- <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a> -->
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  latest
                </div>
              
            

            <div id="docsearch"></div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/config.html">Configurations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/dataloading.html">Data Loading</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/neuron.html">Neuron Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/mito.html">Mitochondria Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/synapse.html">Synapse Detection</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../modules/data.html">connectomics.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../modules/engine.html">connectomics.engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../modules/model.html">connectomics.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../modules/utils.html">connectomics.utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Team</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../about/team.html">About Us</a></li>
</ul>

            
          
        </div>
      </div>

      


    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
      <li>connectomics.model.utils.misc</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for connectomics.model.utils.misc</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.jit.annotations</span> <span class="kn">import</span> <span class="n">Dict</span>


<span class="k">class</span> <span class="nc">IntermediateLayerGetter</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Module wrapper that returns intermediate layers from a model, adapted</span>
<span class="sd">    from https://github.com/pytorch/vision/blob/master/torchvision/models/_utils.py.</span>

<span class="sd">    It has a strong assumption that the modules have been registered</span>
<span class="sd">    into the model in the same order as they are used.</span>
<span class="sd">    This means that one should **not** reuse the same nn.Module</span>
<span class="sd">    twice in the forward if you want this to work.</span>

<span class="sd">    Additionally, it is only able to query submodules that are directly</span>
<span class="sd">    assigned to the model. So if `model` is passed, `model.feature1` can</span>
<span class="sd">    be returned, but not `model.feature1.layer2`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        model (nn.Module): model on which we will extract the features</span>
<span class="sd">        return_layers (Dict[name, new_name]): a dict containing the names</span>
<span class="sd">            of the modules for which the activations will be returned as</span>
<span class="sd">            the key of the dict, and the value of the dict is the name</span>
<span class="sd">            of the returned activation (which the user can specify).</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = torchvision.models.resnet18(pretrained=True)</span>
<span class="sd">        &gt;&gt;&gt; # extract layer1 and layer3, giving as names `feat1` and feat2`</span>
<span class="sd">        &gt;&gt;&gt; new_m = torchvision.models._utils.IntermediateLayerGetter(m,</span>
<span class="sd">        &gt;&gt;&gt;     {&#39;layer1&#39;: &#39;feat1&#39;, &#39;layer3&#39;: &#39;feat2&#39;})</span>
<span class="sd">        &gt;&gt;&gt; out = new_m(torch.rand(1, 3, 224, 224))</span>
<span class="sd">        &gt;&gt;&gt; print([(k, v.shape) for k, v in out.items()])</span>
<span class="sd">        &gt;&gt;&gt;     [(&#39;feat1&#39;, torch.Size([1, 64, 56, 56])),</span>
<span class="sd">        &gt;&gt;&gt;      (&#39;feat2&#39;, torch.Size([1, 256, 14, 14]))]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_version</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="vm">__annotations__</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;return_layers&quot;</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">return_layers</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">set</span><span class="p">(</span><span class="n">return_layers</span><span class="p">)</span><span class="o">.</span><span class="n">issubset</span><span class="p">([</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">()]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;return_layers are not present in model&quot;</span><span class="p">)</span>
        <span class="n">orig_return_layers</span> <span class="o">=</span> <span class="n">return_layers</span>
        <span class="n">return_layers</span> <span class="o">=</span> <span class="p">{</span><span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">return_layers</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
            <span class="n">layers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">return_layers</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">return_layers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">return_layers</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">IntermediateLayerGetter</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">return_layers</span> <span class="o">=</span> <span class="n">orig_return_layers</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_layers</span><span class="p">:</span>
                <span class="n">out_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_layers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="n">out</span><span class="p">[</span><span class="n">out_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="SplitActivation"><a class="viewcode-back" href="../../../../modules/model.html#connectomics.model.utils.SplitActivation">[docs]</a><span class="k">class</span> <span class="nc">SplitActivation</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Apply different activation functions for the outpur tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># number of channels of different target options</span>
    <span class="n">num_channels_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;0&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;1&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s1">&#39;2&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s1">&#39;3&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;4&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s1">&#39;5&#39;</span><span class="p">:</span> <span class="mi">11</span><span class="p">,</span>
        <span class="s1">&#39;6&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">target_opt</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">],</span>
                 <span class="n">output_act</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">split_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">do_cat</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                 <span class="n">do_2d</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">normalize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">output_act</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_opt</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_act</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">do_2d</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_channels_dict</span><span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">split_channels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_opt</span> <span class="o">=</span> <span class="n">target_opt</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_cat</span> <span class="o">=</span> <span class="n">do_cat</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">normalize</span>

        <span class="k">for</span> <span class="n">topt</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_opt</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">topt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;9&#39;</span><span class="p">:</span>
                <span class="n">channels</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">topt</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">split_channels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">channels</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">split_channels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">num_channels_dict</span><span class="p">[</span><span class="n">topt</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">split_only</span> <span class="o">=</span> <span class="n">split_only</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_only</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_act</span><span class="p">(</span><span class="n">output_act</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_channels</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># torch.split returns a tuple</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_only</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span>

        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_apply_act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
             <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_cat</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_get_act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">act</span><span class="p">):</span>
        <span class="n">num_target</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_opt</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">*</span><span class="n">num_target</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">act</span><span class="p">):</span>
            <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_functional_act</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">_apply_act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">act_fn</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">act_fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="ow">and</span> <span class="n">act_fn</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>

        <span class="k">return</span> <span class="n">x</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">build_from_cfg</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span>
                       <span class="n">cfg</span><span class="p">,</span>
                       <span class="n">do_cat</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                       <span class="n">split_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                       <span class="n">normalize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">MODEL</span><span class="o">.</span><span class="n">TARGET_OPT</span><span class="p">,</span>
                   <span class="n">cfg</span><span class="o">.</span><span class="n">INFERENCE</span><span class="o">.</span><span class="n">OUTPUT_ACT</span><span class="p">,</span>
                   <span class="n">split_only</span><span class="o">=</span><span class="n">split_only</span><span class="p">,</span>
                   <span class="n">do_cat</span><span class="o">=</span><span class="n">do_cat</span><span class="p">,</span>
                   <span class="n">do_2d</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">DATASET</span><span class="o">.</span><span class="n">DO_2D</span><span class="p">,</span>
                   <span class="n">normalize</span><span class="o">=</span><span class="n">normalize</span><span class="p">)</span></div>

<span class="c1"># ------------------</span>
<span class="c1"># Swish Activation</span>
<span class="c1"># ------------------</span>
<span class="c1"># An ordinary implementation of Swish function</span>


<span class="k">class</span> <span class="nc">Swish</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># A memory-efficient implementation of Swish function</span>


<span class="k">class</span> <span class="nc">SwishImplementation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_variables</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">sigmoid_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="p">(</span><span class="n">sigmoid_i</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid_i</span><span class="p">)))</span>


<span class="k">class</span> <span class="nc">MemoryEfficientSwish</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">SwishImplementation</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># --------------------</span>
<span class="c1"># Activation Layers</span>
<span class="c1"># --------------------</span>


<span class="k">def</span> <span class="nf">get_activation</span><span class="p">(</span><span class="n">activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Get the specified activation layer. </span>

<span class="sd">    Args:</span>
<span class="sd">        activation (str): one of ``&#39;relu&#39;``, ``&#39;leaky_relu&#39;``, ``&#39;elu&#39;``, ``&#39;gelu&#39;``, </span>
<span class="sd">            ``&#39;swish&#39;``, &#39;efficient_swish&#39;`` and ``&#39;none&#39;``. Default: ``&#39;relu&#39;``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">activation</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">,</span> <span class="s2">&quot;elu&quot;</span><span class="p">,</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;swish&quot;</span><span class="p">,</span> <span class="s2">&quot;efficient_swish&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">],</span> \
        <span class="s2">&quot;Get unknown activation key </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
    <span class="n">activation_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;leaky_relu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;elu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;gelu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
        <span class="s2">&quot;swish&quot;</span><span class="p">:</span> <span class="n">Swish</span><span class="p">(),</span>
        <span class="s2">&quot;efficient_swish&quot;</span><span class="p">:</span> <span class="n">MemoryEfficientSwish</span><span class="p">(),</span>
        <span class="s2">&quot;none&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">activation_dict</span><span class="p">[</span><span class="n">activation</span><span class="p">]</span>


<div class="viewcode-block" id="get_functional_act"><a class="viewcode-back" href="../../../../modules/model.html#connectomics.model.utils.get_functional_act">[docs]</a><span class="k">def</span> <span class="nf">get_functional_act</span><span class="p">(</span><span class="n">activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get the specified activation function. </span>

<span class="sd">    Args:</span>
<span class="sd">        activation (str): one of ``&#39;relu&#39;``, ``&#39;tanh&#39;``, ``&#39;elu&#39;``, ``&#39;sigmoid&#39;``, </span>
<span class="sd">            ``&#39;softmax&#39;`` and ``&#39;none&#39;``. Default: ``&#39;sigmoid&#39;``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">activation</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="s2">&quot;tanh&quot;</span><span class="p">,</span> <span class="s2">&quot;elu&quot;</span><span class="p">,</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span> <span class="s2">&quot;softmax&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">],</span> \
        <span class="s2">&quot;Get unknown activation_fn key </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
    <span class="n">activation_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;relu&#39;</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">relu_</span><span class="p">,</span>
        <span class="s1">&#39;tanh&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
        <span class="s1">&#39;elu&#39;</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">elu_</span><span class="p">,</span>
        <span class="s1">&#39;sigmoid&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">,</span>
        <span class="s1">&#39;softmax&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;none&#39;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">activation_dict</span><span class="p">[</span><span class="n">activation</span><span class="p">]</span></div>

<span class="c1"># ----------------------</span>
<span class="c1"># Normalization Layers</span>
<span class="c1"># ----------------------</span>


<span class="k">def</span> <span class="nf">get_norm_3d</span><span class="p">(</span><span class="n">norm</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bn_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Get the specified normalization layer for a 3D model.</span>

<span class="sd">    Args:</span>
<span class="sd">        norm (str): one of ``&#39;bn&#39;``, ``&#39;sync_bn&#39;`` ``&#39;in&#39;``, ``&#39;gn&#39;`` or ``&#39;none&#39;``.</span>
<span class="sd">        out_channels (int): channel number.</span>
<span class="sd">        bn_momentum (float): the momentum of normalization layers.</span>
<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: the normalization layer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="s2">&quot;sync_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;gn&quot;</span><span class="p">,</span> <span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">],</span> \
        <span class="s2">&quot;Get unknown normalization layer key </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">norm</span><span class="p">)</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;bn&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
        <span class="s2">&quot;sync_bn&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm3d</span><span class="p">,</span>
        <span class="s2">&quot;in&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm3d</span><span class="p">,</span>
        <span class="s2">&quot;gn&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">channels</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span>
        <span class="s2">&quot;none&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span>
    <span class="p">}[</span><span class="n">norm</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="s2">&quot;sync_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;in&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">norm</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">norm</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_norm_2d</span><span class="p">(</span><span class="n">norm</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bn_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Get the specified normalization layer for a 2D model.</span>

<span class="sd">    Args:</span>
<span class="sd">        norm (str): one of ``&#39;bn&#39;``, ``&#39;sync_bn&#39;`` ``&#39;in&#39;``, ``&#39;gn&#39;`` or ``&#39;none&#39;``.</span>
<span class="sd">        out_channels (int): channel number.</span>
<span class="sd">        bn_momentum (float): the momentum of normalization layers.</span>
<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: the normalization layer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="s2">&quot;sync_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;gn&quot;</span><span class="p">,</span> <span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">],</span> \
        <span class="s2">&quot;Get unknown normalization layer key </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">norm</span><span class="p">)</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;bn&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span>
        <span class="s2">&quot;sync_bn&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">,</span>
        <span class="s2">&quot;in&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">,</span>
        <span class="s2">&quot;gn&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">channels</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span>
        <span class="s2">&quot;none&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span>
    <span class="p">}[</span><span class="n">norm</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="s2">&quot;sync_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;in&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">norm</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">norm</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_norm_1d</span><span class="p">(</span><span class="n">norm</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bn_momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Get the specified normalization layer for a 1D model.</span>

<span class="sd">    Args:</span>
<span class="sd">        norm (str): one of ``&#39;bn&#39;``, ``&#39;sync_bn&#39;`` ``&#39;in&#39;``, ``&#39;gn&#39;`` or ``&#39;none&#39;``.</span>
<span class="sd">        out_channels (int): channel number.</span>
<span class="sd">        bn_momentum (float): the momentum of normalization layers.</span>
<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: the normalization layer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="s2">&quot;sync_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;gn&quot;</span><span class="p">,</span> <span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">],</span> \
        <span class="s2">&quot;Get unknown normalization layer key </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">norm</span><span class="p">)</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;bn&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">,</span>
        <span class="s2">&quot;sync_bn&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">,</span>
        <span class="s2">&quot;in&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm1d</span><span class="p">,</span>
        <span class="s2">&quot;gn&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">channels</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span>
        <span class="s2">&quot;none&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">,</span>
    <span class="p">}[</span><span class="n">norm</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;bn&quot;</span><span class="p">,</span> <span class="s2">&quot;sync_bn&quot;</span><span class="p">,</span> <span class="s2">&quot;in&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">norm</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">bn_momentum</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">norm</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_num_params</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">num_param</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">param</span><span class="o">.</span><span class="n">nelement</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()])</span>
    <span class="k">return</span> <span class="n">num_param</span>
</pre></div>

             </article>
             
            </div>
            <footer>
    
  
    
  
      <hr>
  
    
  
    <div role="contentinfo">
      <p>
          &copy; Copyright 2019-2021, Zudi Lin and Donglai Wei.
  
      </p>
    </div>
      
        <div>
          Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
        </div>
       
  
  </footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <!-- commented out for Ignite -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Harvard VCG</h2>
          <p>Visual Computing Group at Harvard</p>
          <a class="with-right-arrow" href="https://lichtmanlab.fas.harvard.edu">Harvard VCG</a>
        </div>
        <div class="col-md-4 text-center">
          <h2>Lichtman Lab</h2>
          <p>Lichtman Lab at Harvard</p>
          <a class="with-right-arrow" href="https://lichtmanlab.fas.harvard.edu">Lichtman Lab</a>
        </div>
        <div class="col-md-4 text-center">
          <h2>CBS</h2>
          <p>Center for Brain Science at Harvard</p>
          <a class="with-right-arrow" href="http://cbs.fas.harvard.edu">CBS</a>
        </div>
      </div>
    </div>
  </div>



  <!-- end of commented out for Ignite -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->
  <!--
  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/index.html" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>
    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>
          <li>
            <a href="#">Features</a>
          </li>
          <li>
            <a href="#">Ecosystem</a>
          </li>
          <li>
            <a href="">Blog</a>
          </li>
          <li>
            <a href="https://zudi-lin.github.io/pytorch_connectomics/build/html/tutorials/snemi.html">Tutorials</a>
          </li>
          <li>
            <a href="https://zudi-lin.github.io/pytorch_connectomics/build/html/index.html">Docs</a>
          </li>
          <li>
            <a href="">Resources</a>
          </li>
          <li>
            <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
  -->
  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    var collapsedSections = ['Notes']
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@docsearch/js@1.0.0-alpha.28/dist/umd/index.min.js"></script>
  <script type="text/javascript">
  let VERSION
  if ('latest'.includes('v')) {
    VERSION = 'latest'
  } else {
    VERSION = 'latest'
  }
  docsearch({
    container: '#docsearch',
    apiKey: '19a7a7a75d87608d6f42c722ed1e293f',
    indexName: 'ignite',
    placeholder: 'Search PyTC Docs',
    searchParameters: {
      'facetFilters': [`version:${VERSION}`],
    }
  });
  </script>
</body>
</html>