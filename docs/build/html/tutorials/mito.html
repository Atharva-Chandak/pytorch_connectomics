



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Mitochondria Segmentation &mdash; connectomics latest documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.svg"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/pytc-theme.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@docsearch/css@1.0.0-alpha.28/dist/style.min.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Synapse Detection" href="synapse.html" />
    <link rel="prev" title="Neuron Segmentation" href="neuron.html" /> 

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">

    <a class="header-logo" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/index.html" aria-label="PyTorch"></a>

    <div class="header-container">

      <div class="main-menu">
        <ul>
          <li>
            <a href="../index.html">Get Started</a>
          </li>
          <li>
            <a href="snemi.html">Tutorials</a>
          </li>
          <li>
            <a href="../index.html">Docs</a>
          </li>
          <li>
            <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master">Github</a>
          </li>
          <li>
            <a href="../about.html">About Us</a>
          </li>

        </ul>
      </div>

      <!-- <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a> -->
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  latest
                </div>
              
            

            <div id="docsearch"></div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/config.html">Configurations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/dataloading.html">Data Loading</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="neuron.html">Neuron Segmentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Mitochondria Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="synapse.html">Synapse Detection</a></li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/data.html">connectomics.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/engine.html">connectomics.engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/model.html">connectomics.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/utils.html">connectomics.utils</a></li>
</ul>
<p class="caption"><span class="caption-text">Team</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../about/team.html">About Us</a></li>
</ul>

            
          
        </div>
      </div>

      


    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Mitochondria Segmentation</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/mito.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="mitochondria-segmentation">
<h1>Mitochondria Segmentation<a class="headerlink" href="#mitochondria-segmentation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Mitochondrion">Mitochondria</a> are the primary energy providers for cell activities, thus essential for metabolism.
Quantification of the size and geometry of mitochondria is not only crucial to basic neuroscience research, but also informative to
clinical studies including, but not limited to, bipolar disorder and diabetes.</p>
<p>This tutorial has two parts. In the first part, you will learn how to make <strong>pixel-wise class prediction</strong> on the widely used benchmark
dataset released by <a class="reference external" href="https://ieeexplore.ieee.org/document/6619103">Lucchi et al.</a> in 2012. In the second part, you will learn how to predict the <strong>instance masks</strong> of
individual mitochondrion from the large-scale MitoEM dataset released by <a class="reference external" href="https://donglaiw.github.io/paper/2020_miccai_mitoEM.pdf">Wei et al.</a> in 2020.</p>
</div>
<div class="section" id="semantic-segmentation">
<h2>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this headline">¶</a></h2>
<p>This section provides step-by-step guidance for mitochondria segmentation with the EM benchmark datasets released by <a class="reference external" href="https://cvlab.epfl.ch/research/page-90578-en-html/research-medical-em-mitochondria-index-php/">Lucchi et al. (2012)</a>.
We consider the task as a <strong>semantic segmentation</strong> task and predict the mitochondria pixels with encoder-decoder ConvNets similar to
the models used in affinity prediction in <a class="reference external" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/tutorials/snemi.html">neuron segmentation</a>. The evaluation of the mitochondria segmentation results is based on the F1 score and Intersection over Union (IoU).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Different from other EM connectomics datasets used in the tutorials, the dataset released by Lucchi et al. is an isotropic dataset,
which means the spatial resolution along all three axes is the same. Therefore a completely 3D U-Net and data augmentation along z-x
and z-y planes besides x-y planes are preferred.</p>
</div>
<p>All the scripts needed for this tutorial can be found at <code class="docutils literal notranslate"><span class="pre">pytorch_connectomics/scripts/</span></code>. Need to pass the argument <code class="docutils literal notranslate"><span class="pre">--config-file</span> <span class="pre">configs/Lucchi-Mitochondria.yaml</span></code> during training and inference to load the required configurations for this task.
The pytorch dataset class of lucchi data is <a class="reference internal" href="../modules/data.html#connectomics.data.dataset.VolumeDataset" title="connectomics.data.dataset.VolumeDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">connectomics.data.dataset.VolumeDataset</span></code></a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/lucchi_qual.png"><img alt="../_images/lucchi_qual.png" src="../_images/lucchi_qual.png" style="width: 800px;" /></a>
</div>
<p>Qualitative results of the model prediction on the mitochondria segmentation dataset released by
Lucchi et al., without any post-processing.</p>
<div class="section" id="get-the-data">
<h3>1 - Get the data<a class="headerlink" href="#get-the-data" title="Permalink to this headline">¶</a></h3>
<p>Download the dataset from our server:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>wget http://rhoana.rc.fas.harvard.edu/dataset/lucchi.zip
</pre></div>
</div>
<p>For description of the data please check <a class="reference external" href="https://www.epfl.ch/labs/cvlab/data/data-em/">the author page</a>.</p>
</div>
<div class="section" id="run-training">
<h3>2 - Run training<a class="headerlink" href="#run-training" title="Permalink to this headline">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>source activate py3_torch
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u scripts/main.py \
--config-file configs/Lucchi-Mitochondria.yaml
</pre></div>
</div>
</div>
<div class="section" id="visualize-the-training-progress">
<h3>4 - Visualize the training progress<a class="headerlink" href="#visualize-the-training-progress" title="Permalink to this headline">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir runs
</pre></div>
</div>
</div>
<div class="section" id="inference-on-test-data">
<h3>5 - Inference on test data<a class="headerlink" href="#inference-on-test-data" title="Permalink to this headline">¶</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>source activate py3_torch
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -u scripts/main.py \
--config-file configs/Lucchi-Mitochondria.yaml --inference \
--checkpoint outputs/Lucchi_mito_baseline/volume_100000.pth.tar
</pre></div>
</div>
</div>
<div class="section" id="run-evaluation">
<h3>6 - Run evaluation<a class="headerlink" href="#run-evaluation" title="Permalink to this headline">¶</a></h3>
<p>Since the ground-truth label of the test set is public, we can run the evaluation locally:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">connectomics.utils.evaluation</span> <span class="kn">import</span> <span class="n">get_binary_jaccard</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">/</span> <span class="mf">255.</span> <span class="c1"># output is casted to uint8 with range [0,255].</span>
<span class="n">gt</span> <span class="o">=</span> <span class="p">(</span><span class="n">gt</span><span class="o">!==</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">thres</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span> <span class="c1"># evaluate at multiple thresholds.</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">get_binary_jaccard</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">gt</span><span class="p">,</span> <span class="n">thres</span><span class="p">)</span>
</pre></div>
</div>
<p>The prediction can be further improved by conducting median filtering to remove noise:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">connectomics.utils.evaluate</span> <span class="kn">import</span> <span class="n">get_binary_jaccard</span>
<span class="kn">from</span> <span class="nn">connectomics.utils.process</span> <span class="kn">import</span> <span class="n">binarize_and_median</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">/</span> <span class="mf">255.</span> <span class="c1"># output is casted to uint8 with range [0,255].</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">binarize_and_median</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span> <span class="n">thres</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">gt</span> <span class="o">=</span> <span class="p">(</span><span class="n">gt</span><span class="o">!==</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">get_binary_jaccard</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span> <span class="c1"># prediction is already binarized</span>
</pre></div>
</div>
<p>Our pretained model achieves a foreground IoU and IoU of <strong>0.892</strong> and <strong>0.943</strong> on the test set, respectively. The results are better or on par with
state-of-the-art approaches. Please check <a class="reference external" href="https://github.com/zudi-lin/pytorch_connectomics/blob/master/BENCHMARK.md">BENCHMARK.md</a>  for detailed performance
comparison and the pre-trained models.</p>
</div>
</div>
<div class="section" id="instance-segmentation">
<h2>Instance Segmentation<a class="headerlink" href="#instance-segmentation" title="Permalink to this headline">¶</a></h2>
<p>This section provides step-by-step guidance for mitochondria segmentation with our benchmark datasets <a class="reference external" href="https://donglaiw.github.io/page/mitoEM/index.html">MitoEM</a>.
We consider the task as 3D <strong>instance segmentation</strong> task and provide three different confiurations of the model output.
The model is <code class="docutils literal notranslate"><span class="pre">unet_res_3d</span></code>, similar to the one used in <a class="reference external" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/tutorials/snemi.html">neuron segmentation</a>.
The evaluation of the segmentation results is based on the AP-75 (average precision with an IoU threshold of 0.75).</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/mito_complex.png"><img alt="../_images/mito_complex.png" src="../_images/mito_complex.png" style="width: 800px;" /></a>
</div>
<p>Complex mitochondria in the MitoEM dataset:(<strong>a</strong>) mitochondria-on-a-string (MOAS), and (<strong>b</strong>) dense tangle of touching instances.
Those challenging cases are prevalent but not covered in previous datasets.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The MitoEM dataset has two sub-datasets <strong>MitoEM-Rat</strong> and <strong>MitoEM-Human</strong> based on the source of the tissues. Three training configuration files on <strong>MitoEM-Rat</strong>
are provided in <code class="docutils literal notranslate"><span class="pre">pytorch_connectomics/configs/MitoEM/</span></code> for different learning setting as described in this <a class="reference external" href="https://donglaiw.github.io/paper/2020_miccai_mitoEM.pdf">paper</a>.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Since the dataset is very large and can not be directly loaded into memory, we use the <a class="reference internal" href="../modules/data.html#connectomics.data.dataset.TileDataset" title="connectomics.data.dataset.TileDataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">connectomics.data.dataset.TileDataset</span></code></a> dataset class that only
loads part of the whole volume by opening involved <code class="docutils literal notranslate"><span class="pre">PNG</span></code> or <code class="docutils literal notranslate"><span class="pre">TIFF</span></code> images.</p>
</div>
<ol class="arabic">
<li><p>Introduction to the dataset:</p>
<blockquote>
<div><p>On the Harvard RC cluster, the datasets can be found at:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>/n/pfister_lab2/Lab/vcg_connectomics/mitochondria/miccai2020/rat
</pre></div>
</div>
<p>and</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>/n/pfister_lab2/Lab/vcg_connectomics/mitochondria/miccai2020/human
</pre></div>
</div>
<p>For the <strong>public link</strong> of the dataset, check the <a class="reference external" href="https://donglaiw.github.io/page/mitoEM/index.html">project page</a>.</p>
<p>Dataset description:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">im</span></code>: includes 1,000 single-channel <code class="docutils literal notranslate"><span class="pre">*.png</span></code> files (<strong>4096x4096</strong>) of raw EM images (with a spatial resolution of <strong>30x8x8</strong> nm).
The 1,000 images are splited into 400, 100 and 500 slices for training, validation and inference, respectively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mito</span></code>: includes 500 single-channel <code class="docutils literal notranslate"><span class="pre">*.png</span></code> files (<strong>4096x4096</strong>) of instance labels. The files are
splited into 400 and 100 slices for training and validation. The ground-truth annotation of the test set (rest 500 slices)
is not publicly provided but can be evaluated online at the <a class="reference external" href="https://mitoem.grand-challenge.org">MitoEM challenge page</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*.json</span></code>: dictionary contains paths to the <code class="docutils literal notranslate"><span class="pre">*.png</span></code> files and metadata of the datasets.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Configure <code class="docutils literal notranslate"><span class="pre">*.yaml</span></code> files for different learning targets:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MitoEM-R-A.yaml</span></code>: output 3 channels for predicting the affinty between voxels.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MitoEM-R-AC.yaml</span></code>: output 4 channels for predicting both affinity and instance contour.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MitoEM-R-BC.yaml</span></code>: output 2 channels for predicting both the binary foreground mask and instance contour. This configuration achieves the
best overall performance according to our <a class="reference external" href="https://donglaiw.github.io/paper/2020_miccai_mitoEM.pdf">experiments</a>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Run training script for the <strong>U3D-BC</strong> model:</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default the path of images and labels are not specified. To
run the training scripts, please revise the <code class="docutils literal notranslate"><span class="pre">DATASET.IMAGE_NAME</span></code>, <code class="docutils literal notranslate"><span class="pre">DATASET.LABEL_NAME</span></code>, <code class="docutils literal notranslate"><span class="pre">DATASET.OUTPUT_PATH</span></code>
and <code class="docutils literal notranslate"><span class="pre">DATASET.INPUT_PATH</span></code> options in <code class="docutils literal notranslate"><span class="pre">configs/MitoEM-R-*.yaml</span></code>.
The options can also be given as command-line arguments without changing of the <code class="docutils literal notranslate"><span class="pre">yaml</span></code> configuration files.</p>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ source activate py3_torch
$ python -u scripts/main.py --config-file configs/MitoEM-R-BC.yaml
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Visualize the training progress. More info <a class="reference external" href="https://vcg.github.io/newbie-wiki/build/html/computation/machine_rc.html">here</a>:</p>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ tensorboard --logdir outputs/MitoEM_R_BC/
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Run inference on validation/test image volumes (suppose the model is optimized for 100k iterations):</p>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>$ source activate py3_torch
$ python -u scripts/main.py \
  --config-file configs/MitoEM-R-BC.yaml --inference \
  --checkpoint outputs/MitoEM_R_BC/checkpoint_100000.pth.tar
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please change the <code class="docutils literal notranslate"><span class="pre">INFERENCE.IMAGE_NAME</span></code> <code class="docutils literal notranslate"><span class="pre">INFERENCE.OUTPUT_PATH</span></code> <code class="docutils literal notranslate"><span class="pre">INFERENCE.OUTPUT_NAME</span></code>
options in <code class="docutils literal notranslate"><span class="pre">configs/MitoEM-R-*.yaml</span></code>.</p>
</div>
</div></blockquote>
</li>
<li><p>Merge output volumes and run watershed segmentation:</p>
<blockquote>
<div><p>As mentioned before, the dataset is very large and can hardly be directly loaded into memory for
processing. Therefore our code run prediction on smaller chunks sequentially, which produces
multiple <code class="docutils literal notranslate"><span class="pre">*.h5</span></code> files with the coordinate information. To merge the chunks into a single volume
and apply the segmentation algorithm:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">connectomics.data.utils</span> <span class="kn">import</span> <span class="n">readvol</span>
<span class="kn">from</span> <span class="nn">connectomics.utils.process</span> <span class="kn">import</span> <span class="n">bc_watershed</span>

<span class="n">output_files</span> <span class="o">=</span> <span class="s1">&#39;outputs/MitoEM_R_BC/test/*.h5&#39;</span> <span class="c1"># output folder with chunks</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">output_files</span><span class="p">)</span>

<span class="n">vol_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">4096</span><span class="p">)</span> <span class="c1"># MitoEM test set</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">vol_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;process chunk: &quot;</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">pos</span><span class="p">))</span>
    <span class="n">chunk</span> <span class="o">=</span> <span class="n">readvol</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">pred</span><span class="p">[:,</span> <span class="n">pos</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="n">pos</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="n">pos</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">pos</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span><span class="n">pos</span><span class="p">[</span><span class="mi">5</span><span class="p">]]</span> <span class="o">=</span> <span class="n">chunk</span>

<span class="c1"># This function process the array in numpy.float64 format.</span>
<span class="c1"># Please allocate enough memory for processing.</span>
<span class="n">segm</span> <span class="o">=</span> <span class="n">bc_watershed</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">thres1</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">thres2</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">thres3</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">thres_small</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</pre></div>
</div>
<p>Then the segmentation map should be ready to be submitted to the MitoEM challenge website for
evaluation. Please note that this tutorial only take the <strong>MitoEM-Rat</strong> set as an example. The
<strong>MitoEM-Human</strong> set also need to be segmented for online evaluation.</p>
</div></blockquote>
</li>
</ol>
</div>
</div>


             </article>
             
            </div>
            <footer>
    
      <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        
          <a href="synapse.html" class="btn btn-neutral float-right" title="Synapse Detection" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
        
        
          <a href="neuron.html" class="btn btn-neutral" title="Neuron Segmentation" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
        
      </div>
    
  
    
  
      <hr>
  
    
  
    <div role="contentinfo">
      <p>
          &copy; Copyright 2019-2021, Zudi Lin and Donglai Wei.
  
      </p>
    </div>
      
        <div>
          Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
        </div>
       
  
  </footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Mitochondria Segmentation</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#semantic-segmentation">Semantic Segmentation</a><ul>
<li><a class="reference internal" href="#get-the-data">1 - Get the data</a></li>
<li><a class="reference internal" href="#run-training">2 - Run training</a></li>
<li><a class="reference internal" href="#visualize-the-training-progress">4 - Visualize the training progress</a></li>
<li><a class="reference internal" href="#inference-on-test-data">5 - Inference on test data</a></li>
<li><a class="reference internal" href="#run-evaluation">6 - Run evaluation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#instance-segmentation">Instance Segmentation</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <!-- commented out for Ignite -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Harvard VCG</h2>
          <p>Visual Computing Group at Harvard</p>
          <a class="with-right-arrow" href="https://lichtmanlab.fas.harvard.edu">Harvard VCG</a>
        </div>
        <div class="col-md-4 text-center">
          <h2>Lichtman Lab</h2>
          <p>Lichtman Lab at Harvard</p>
          <a class="with-right-arrow" href="https://lichtmanlab.fas.harvard.edu">Lichtman Lab</a>
        </div>
        <div class="col-md-4 text-center">
          <h2>CBS</h2>
          <p>Center for Brain Science at Harvard</p>
          <a class="with-right-arrow" href="http://cbs.fas.harvard.edu">CBS</a>
        </div>
      </div>
    </div>
  </div>



  <!-- end of commented out for Ignite -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->
  <!--
  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://zudi-lin.github.io/pytorch_connectomics/build/html/index.html" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>
    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>
          <li>
            <a href="#">Features</a>
          </li>
          <li>
            <a href="#">Ecosystem</a>
          </li>
          <li>
            <a href="">Blog</a>
          </li>
          <li>
            <a href="https://zudi-lin.github.io/pytorch_connectomics/build/html/tutorials/snemi.html">Tutorials</a>
          </li>
          <li>
            <a href="https://zudi-lin.github.io/pytorch_connectomics/build/html/index.html">Docs</a>
          </li>
          <li>
            <a href="">Resources</a>
          </li>
          <li>
            <a href="https://github.com/zudi-lin/pytorch_connectomics/tree/master">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>
  -->
  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    var collapsedSections = ['Notes']
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/@docsearch/js@1.0.0-alpha.28/dist/umd/index.min.js"></script>
  <script type="text/javascript">
  let VERSION
  if ('latest'.includes('v')) {
    VERSION = 'latest'
  } else {
    VERSION = 'latest'
  }
  docsearch({
    container: '#docsearch',
    apiKey: '19a7a7a75d87608d6f42c722ed1e293f',
    indexName: 'ignite',
    placeholder: 'Search PyTC Docs',
    searchParameters: {
      'facetFilters': [`version:${VERSION}`],
    }
  });
  </script>
</body>
</html>